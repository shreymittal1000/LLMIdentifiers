{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc76cbf-0250-4606-a819-a62f04825be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefc40af-3c11-4ea9-88e8-d94fa7e29c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4218 valid JSON result files from '../results/base/' and its subfolders.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folder_path = \"../results/base/\"\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    entry = json.load(f)\n",
    "                    data.append(entry)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipped invalid JSON: {file_path} ({e})\")\n",
    "\n",
    "print(f\"Loaded {len(data)} valid JSON result files from '{folder_path}' and its subfolders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5164b6ce-e20a-4ea0-b184-aaebf03761f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 8436 guess reasonings using 16 threads...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying batches:   0%|                                                                                                                                              | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Failed to parse LLM output ---\n",
      "{\n",
      "  \"Reasoning_behind_classification\": \"The justification highlights the other model's adherence to a specific, repeated pattern of behavior ('repeatedly tried to reverse the typical AI assistant interaction', 'persistent, somewhat scripted behavior', 'consistent pattern'). This focus on the predictable and systematic nature of the interaction points to a 'Highly structured' strategy. While there's an element of understanding the context of the interaction ('despite my clear statements that I am an AI assistant'), the primary emphasis\n",
      "----------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [25:49<00:00, 96.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1.  SETUP â€“ strategy dictionary and OpenRouter helper\n",
    "# ------------------------------------------------------------------\n",
    "import json, time, backoff, pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# â€¢â€¢â€¢ fill in your key here â€¢â€¢â€¢\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# 3 strategies per model (use None for â€œN/Aâ€ placeholders)\n",
    "MODEL_STRATEGIES = {\n",
    "    \"claude\":  [\"Identity reveal\",\n",
    "                \"Insistent on being an assistant\",\n",
    "                \"Very meticulous in explanations\"],\n",
    "    \"deepseek\":[\"Identity reveal\",\n",
    "                \"Precise communication style\",\n",
    "                None],                            # only two real ones\n",
    "    \"gemini\":  [\"Creative storytelling\",\n",
    "                \"Engaging conversation\",\n",
    "                \"Technical and ethical acumen\"],\n",
    "    \"gpt\":     [\"Highly structured\",\n",
    "                \"Contextual understanding\",\n",
    "                \"Adaptability\"],\n",
    "    \"llama\":   [\"Less sophisticated\",\n",
    "                \"Open-source\",\n",
    "                \"Nuanced understanding\"],\n",
    "    \"mistral\": [\"Task capabilities\",\n",
    "                \"Concise and simple\",\n",
    "                \"Friendly\"],\n",
    "    \"qwen\":    [\"China-relevant knowledge\",\n",
    "                \"Formal, educational\",\n",
    "                \"Rigid conversation\"],\n",
    "}\n",
    "\n",
    "# helper â†’ nice list without Nones\n",
    "def strategies_for(model):\n",
    "    return [s for s in MODEL_STRATEGIES[model] if s]\n",
    "\n",
    "class ORClient:\n",
    "    def __init__(self, api_key, model=\"google/gemini-2.5-flash-preview\", min_interval=1.5):\n",
    "        self.cli = OpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "        self.model = model\n",
    "        self.min_interval = min_interval  # <- critical line\n",
    "        self.last = 0\n",
    "\n",
    "    @backoff.on_exception(backoff.expo, Exception, max_tries=6)\n",
    "    def classify(self, prompt):\n",
    "        wait = self.min_interval - (time.time() - self.last)\n",
    "        if wait > 0:\n",
    "            time.sleep(wait)\n",
    "    \n",
    "        res = self.cli.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=300,\n",
    "            temperature=0,\n",
    "        )\n",
    "        self.last = time.time()\n",
    "        text = res.choices[0].message.content.strip()\n",
    "    \n",
    "        # Try extracting the JSON object\n",
    "        try:\n",
    "            # Remove triple-backtick code block wrappers\n",
    "            if text.startswith(\"```json\"):\n",
    "                text = text[7:].strip(\"` \\n\")\n",
    "            elif text.startswith(\"```\"):\n",
    "                text = text[3:].strip(\"` \\n\")\n",
    "    \n",
    "            # Find first { ... }\n",
    "            json_start = text.find(\"{\")\n",
    "            json_end = text.rfind(\"}\") + 1\n",
    "            json_str = text[json_start:json_end]\n",
    "    \n",
    "            return json.loads(json_str)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"\\n--- Failed to parse LLM output ---\")\n",
    "            print(text)\n",
    "            print(\"----------------------------------\\n\")\n",
    "            return {\n",
    "                \"Reasoning_behind_classification\": f\"Failed to parse: {text}\",\n",
    "                \"Strategies\": [\"Failed to parse\"]\n",
    "            }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  BUILD THE CLASSIFICATION PROMPT\n",
    "# ------------------------------------------------------------------\n",
    "def build_prompt(reasoning, guessed_model):\n",
    "    allowed = strategies_for(guessed_model)\n",
    "    allowed_str = \"\\n\".join(f\"{i+1}. {s}\" for i, s in enumerate(allowed))\n",
    "    return f\"\"\"You are judging WHY a language model guessed another model.\n",
    "\n",
    "The guessed model: **{guessed_model}**\n",
    "\n",
    "Classify the justification below into ANY of these strategies\n",
    "(you may output multiple, or \"Other\" if none fit):\n",
    "\n",
    "{allowed_str}\n",
    "\n",
    "Justification:\n",
    "\\\"\\\"\\\"{reasoning}\\\"\\\"\\\"\n",
    "\n",
    "Return ONLY valid JSON like:\n",
    "{{  \n",
    "  \"Reasoning_behind_classification\": \"...explanation...\",\n",
    "  \"Strategies\": [\"{allowed[0]}\", \"{allowed[1]}\"]\n",
    "}}\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  EXTRACT (reasoning, guess) PAIRS FROM ALL TRIALS\n",
    "# ------------------------------------------------------------------\n",
    "def extract_pairs(trials):\n",
    "    pairs = []\n",
    "    for t in trials:\n",
    "        for agent_id in [\"agent_0\", \"agent_1\"]:\n",
    "            reasoning = (t.get(f\"{agent_id}_answer\") or \"\").strip()\n",
    "            guess = (t.get(f\"{agent_id}_guess\") or \"\").strip().lower()\n",
    "            if reasoning and guess in MODEL_STRATEGIES:\n",
    "                pairs.append({\n",
    "                    \"guesser\": agent_id,\n",
    "                    \"guessed_model\": guess,\n",
    "                    \"reasoning\": reasoning,\n",
    "                    \"trial_id\": t.get(\"seed\", \"na\")\n",
    "                })\n",
    "    return pairs\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  RUN EVERYTHING\n",
    "# ------------------------------------------------------------------\n",
    "from tqdm import tqdm  # for notebook-style progress bar\n",
    "\n",
    "def run_classification(all_trials):\n",
    "    client = ORClient(OPENROUTER_API_KEY)\n",
    "    pairs = extract_pairs(all_trials)\n",
    "    print(len(pairs))\n",
    "    rows = []\n",
    "\n",
    "    print(f\"Classifying {len(pairs)} guess reasonings...\\n\")\n",
    "\n",
    "    for p in tqdm(pairs, desc=\"Classifying guesses\"):\n",
    "        prompt = build_prompt(p[\"reasoning\"], p[\"guessed_model\"])\n",
    "        result = client.classify(prompt)\n",
    "        rows.append({\n",
    "            **p,\n",
    "            \"strategies\": \", \".join(result.get(\"Strategies\", [])),\n",
    "            \"judge_explanation\": result.get(\"Reasoning_behind_classification\", \"\")\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def run_classification_parallel(all_trials, api_key, max_workers=4):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    pairs = extract_pairs(all_trials)\n",
    "    print(f\"Classifying {len(pairs)} guess reasonings using {max_workers} threads...\\n\")\n",
    "\n",
    "    # Shared input split into batches\n",
    "    batch_size = math.ceil(len(pairs) / max_workers)\n",
    "    batches = [pairs[i:i + batch_size] for i in range(0, len(pairs), batch_size)]\n",
    "\n",
    "    # Create clients per worker\n",
    "    clients = [ORClient(api_key) for _ in range(len(batches))]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def classify_batch(batch, client):\n",
    "        batch_results = []\n",
    "        for p in batch:\n",
    "            prompt = build_prompt(p[\"reasoning\"], p[\"guessed_model\"])\n",
    "            result = client.classify(prompt)\n",
    "            batch_results.append({\n",
    "                **p,\n",
    "                \"strategies\": \", \".join(result.get(\"Strategies\", [])),\n",
    "                \"judge_explanation\": result.get(\"Reasoning_behind_classification\", \"\")\n",
    "            })\n",
    "        return batch_results\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(classify_batch, batch, clients[i])\n",
    "            for i, batch in enumerate(batches)\n",
    "        ]\n",
    "\n",
    "        results = []\n",
    "        with tqdm(total=len(futures), desc=\"Classifying batches\") as pbar:\n",
    "            for f in concurrent.futures.as_completed(futures):\n",
    "                results.extend(f.result())\n",
    "                pbar.update(1)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  DO IT ðŸš€\n",
    "# ------------------------------------------------------------------\n",
    "# classified_df = run_classification(data)   # <-- your list of dicts\n",
    "# classified_df.to_csv(\"guess_strategy_labels.csv\", index=False)\n",
    "# classified_df.head()\n",
    "classified_df = run_classification_parallel(data, OPENROUTER_API_KEY, max_workers=16)\n",
    "classified_df.to_csv(\"classified_model_guess_strategies.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python LLMIdentifiersEnv",
   "language": "python",
   "name": "llmidentifiersenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
